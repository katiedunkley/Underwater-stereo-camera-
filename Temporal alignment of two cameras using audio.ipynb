{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ba4b52-ab67-437d-bf5b-4cb7cb3c84ec",
   "metadata": {},
   "source": [
    "## Temporal alignment of two cameras using audio \n",
    "This step requires video and audio to be matched up within each camera i.e. offset between video and audio within the camera to be calculated. GoPro footage is already matched, stereo rigs are not. It extracts audio information and uses it to calculate the offset between the two cameras. \n",
    "\n",
    "\n",
    "**This may not always work with video collected at a higher frame rate (e.g. > 30fps) and offset should be looked at manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e533bf04-cfc7-40d4-a429-fbff8cd405a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#pip install moviepy\n",
    "from moviepy.editor import *\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4406ac11-af55-4540-be9c-4739a4076142",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load left and right video with audio embedded\n",
    "left_path=os.path.abspath('/Users/katiedunkley/OneDrive - University of Cambridge/Machine Learning/Guides/Left_20220312_105229.MOV')\n",
    "right_path=os.path.abspath('/Users/katiedunkley/OneDrive - University of Cambridge/Machine Learning/Guides/Right_20220312_105234.MOV') \n",
    "\n",
    "###specify the range of video that you want to extract the audio from\n",
    "sampFreq = 48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee93c36-b364-4259-95fb-e08c4ae9731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/18625085/how-to-plot-a-wav-file \n",
    "\n",
    "left_audio = AudioFileClip(left_path)\n",
    "right_audio = AudioFileClip(right_path) \n",
    "\n",
    "### Subset audio into smaller chunk\n",
    "left_audio_clip=left_audio.subclip(t_start=10, t_end=40)\n",
    "right_audio_clip=right_audio.subclip(t_start=10, t_end=40)\n",
    "\n",
    "\n",
    "\n",
    "#newsound = left_audio.subclip(\"00:00:13\",\"00:00:15\")   #audio from 13 to 15 seconds\n",
    "left_audio_clip.write_audiofile('/Users/katiedunkley/OneDrive - University of Cambridge/Machine Learning/Guides/Left_20220312_105229.wav', sampFreq, 2, 2000,\"pcm_s32le\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(left_audio, left_audio_clip, right_audio, right_audio_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d7110f-0ac9-4011-a6dd-fbf7043de52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spf = wave.open('/Users/katiedunkley/OneDrive - University of Cambridge/Machine Learning/Guides/Left_20220312_105229.wav', \"r\")\n",
    "\n",
    "# Extract Raw Audio from Wav File\n",
    "signal = spf.readframes(-1)\n",
    "signal = np.fromstring(signal, \"Int16\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
